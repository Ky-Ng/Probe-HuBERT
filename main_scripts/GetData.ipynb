{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting HuBERT Hidden Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleng/B_Organized/A_School/Ling_487/clean_code/Probe-HuBERT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kyleng/B_Organized/A_School/Ling_487/clean_code/Probe-HuBERT/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "from transformers import HubertForCTC, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict \n",
    "\n",
    "# Custom Helper Libraries\n",
    "from helper_scripts.TenseLax import TenseLax\n",
    "from helper_scripts.AudioProcessing import AudioProcessing\n",
    "from helper_scripts.Constants import *\n",
    "from helper_scripts.Pathing import Pathing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap\n",
    "```\n",
    "let n = number of samples of audio files\n",
    "let s = number of segments/phonemes to be segmented\n",
    "let k = number of encoders in LLM\n",
    "let l[i] = number of speech vectors per input sequence\n",
    "```\n",
    "\n",
    "1) Select n=200 samples of audio files from a specific subset of dialects from `TIMIT`\n",
    "2) Create the output `hidden_states`\n",
    "    - s entries\n",
    "    - each entry is a size k=25 array \n",
    "    - each array index is a numpy array representing `l[i]` speech vectors--where each speech vector is of size 1024\n",
    "3) Load in each of the n samples using `librosa`\n",
    "4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import Audio Files\n",
    "- Extract 200 path's to audio samples to save computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 4969 speech samples\n",
      "Succesfully randomly sampled 200 speech samples\n"
     ]
    }
   ],
   "source": [
    "# Path is TIMIT/<TEST or TRAIN>/<DIALECT>/<SPEAKER ID>/<SEGMENT ID>.wav\n",
    "DATASET_PATH = \"../Timit-Database/TIMIT/\"\n",
    "ALL_WAVS_PATH = os.path.join(DATASET_PATH, \"*\", \"*\", \"*\", \"*.wav\")\n",
    "\n",
    "speech_paths = glob.glob(ALL_WAVS_PATH)\n",
    "print(f\"Importing {len(speech_paths)} speech samples\")\n",
    "\n",
    "speech_paths = AudioProcessing.select_samples(\n",
    "    speech_paths,\n",
    "    num_samples=Constants.EXPERIMENTATION.NUM_SPEECH_SAMPLES\n",
    ")\n",
    "\n",
    "print(f\"Succesfully randomly sampled {len(speech_paths)} speech samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Create the Data Structure for Saving the Hidden States\n",
    "\n",
    "```\n",
    "For Each Phoneme\n",
    "    For Each Encoder \n",
    "        For Each Sequence of 1024 Vectors\n",
    "            Append Hidden State\n",
    "```\n",
    "\n",
    "Hidden States maps <`phoneme string`, a list of`Hidden State Representation`>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hidden_states = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Calculate Boundaries for each Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in speech_paths:\n",
    "    # Step 1) Generate the hidden states and boundaries\n",
    "    embedded_audio, num_speech_frames, sequence_length = AudioProcessing.process_audio(\n",
    "        wav_path=path,\n",
    "        embedding_model=Constants.EXPERIMENTATION.EMBEDDING_MODEL,\n",
    "        inference_model=Constants.EXPERIMENTATION.INFERENCE_MODEL,\n",
    "        sampling_rate=16000\n",
    "    )\n",
    "\n",
    "    scaled_segmentation = AudioProcessing.get_sequence_boundary(\n",
    "        TIMIT_wav_path=path,\n",
    "        num_speech_frames=num_speech_frames,\n",
    "        num_speech_vec=sequence_length\n",
    "    )\n",
    "\n",
    "    # Step 2) Select boundaries for matching phonemes\n",
    "    filtered_segmentation = AudioProcessing.filter_segmentation(\n",
    "        combined_df=scaled_segmentation,\n",
    "        desired_phonemes=TenseLax.getSet()\n",
    "    )\n",
    "\n",
    "    # Step 3) Place Hidden State into output matrix\n",
    "    for row in filtered_segmentation.itertuples():\n",
    "        _, seq_start_vec_idx, seq_end_vec_idx, phoneme = row\n",
    "        \n",
    "        # Step 3a) Get the Hidden States per encoder for the entire speech segment\n",
    "        utterance_hidden_states = AudioProcessing.get_hidden_states(\n",
    "            input_embedding=embedded_audio,\n",
    "            inference_model=Constants.EXPERIMENTATION.INFERENCE_MODEL,\n",
    "            start_idx=seq_start_vec_idx,\n",
    "            end_idx=seq_end_vec_idx \n",
    "        )\n",
    "\n",
    "        # Step 3b) Append hidden States to the existing hidden states for this row\n",
    "        all_hidden_states[phoneme].append(\n",
    "            utterance_hidden_states\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Save Phoneme Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ao: (25, 857, 1024)\n",
      "ey: (25, 635, 1024)\n",
      "iy: (25, 1453, 1024)\n",
      "ae: (25, 1483, 1024)\n",
      "ow: (25, 555, 1024)\n",
      "eh: (25, 790, 1024)\n",
      "ih: (25, 877, 1024)\n",
      "uh: (25, 62, 1024)\n",
      "uw: (25, 188, 1024)\n"
     ]
    }
   ],
   "source": [
    "for phoneme, hidden_state in all_hidden_states.items():\n",
    "    combined_per_segment = np.concatenate(hidden_state, axis=1)\n",
    "    print(f\"{phoneme}: {combined_per_segment.shape}\")\n",
    "    Pathing.save_file_np(\n",
    "        save_dir=Constants.PATHING.hidden_state_save_path,\n",
    "        save_file_name=f\"HS_{phoneme}_{combined_per_segment.shape[1]}.npy\",\n",
    "        to_save=combined_per_segment\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
